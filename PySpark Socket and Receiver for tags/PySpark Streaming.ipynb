{"cells":[{"cell_type":"code","execution_count":null,"id":"42c3feb8","metadata":{"id":"42c3feb8","outputId":"58aa959f-71bd-4daa-c097-fe41b05bf608"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/12/02 05:11:02 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/12/02 05:11:02 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/12/02 05:11:02 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/12/02 05:11:02 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","[Stage 0:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------\n","Time: 2022-12-02 05:11:10\n","-------------------------------------------\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------\n","Time: 2022-12-02 05:11:10\n","-------------------------------------------\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 0:>                                                          (0 + 1) / 1]\r"]}],"source":["from pyspark import SparkConf,SparkContext\n","from pyspark.streaming import StreamingContext\n","from pyspark.sql import Row,SQLContext\n","import sys\n","import requests\n","import time\n","import subprocess\n","import re\n","from google.cloud import bigquery\n","import pandas as pd\n","\n","# global variables\n","bucket = \"YOUR GCP BUCKET\"    # TODO : replace with your own bucket name\n","output_directory_wordcount = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/wordcount'.format(bucket)\n","\n","# output table and columns name\n","project_id = 'YOUR PROJECT ID'\n","output_dataset = 'YOUR OUTPUT DATASET'                     #the name of your dataset in BigQuery\n","output_table_wordcount = 'YOUR OUTPUT TABLE NAME'\n","columns_name_wordcount = ['tag', 'count', 'date']\n","\n","# parameter\n","IP = 'localhost'    # ip port\n","PORT = 9001         # port\n","\n","STREAMTIME = 600     # time that the streaming process runs\n","\n","\n","# Helper functions\n","def saveToStorage(rdd, output_directory, columns_name, mode):\n","    \"\"\"\n","    Save each RDD in this DStream to google storage\n","    Args:\n","    rdd: input rdd\n","    output_directory: output directory in google storage\n","    columns_name: columns name of dataframe\n","    mode: mode = \"overwirte\", overwirte the file\n","    mode = \"append\", append data to the end of file\n","    \"\"\"\n","    if not rdd.isEmpty():\n","        (rdd.toDF( columns_name ) \\\n","        .write.save(output_directory, format=\"json\", mode=mode))\n","\n","\n","def saveToBigQuery(sc, output_dataset, output_table, directory):\n","    \"\"\"\n","    Put temp streaming json files in google storage to google BigQuery\n","    and clean the output files in google storage\n","    \"\"\"\n","    files = directory + '/part-*'\n","    subprocess.check_call(\n","        'bq load --source_format NEWLINE_DELIMITED_JSON '\n","        '--replace '\n","        '--autodetect '\n","        '{dataset}.{table} {files}'.format(\n","            dataset=output_dataset, table=output_table, files=files\n","        ).split())\n","    output_path = sc._jvm.org.apache.hadoop.fs.Path(directory)\n","    output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(output_path, True)\n","\n","\n","def wordCount(words):\n","    \"\"\"\n","    Calculte the count of 5 sepcial words for every 60 seconds (window no overlap)\n","    You can choose your own words.\n","    Your should:\n","    1. filter the words\n","    2. count the word during a special window size\n","    3. add a time related mark to the output of each window, ex: a datetime type\n","    Hints:\n","        You can take a look at reduceByKeyAndWindow transformation\n","        Dstream is a serioes of rdd, each RDD in a DStream contains data from a certain interval\n","        You may want to take a look of transform transformation of DStream when trying to add a time\n","    Args:\n","        dstream(DStream): stream of real time tweets\n","    Returns:\n","        DStream Object with inner structure (word, (count, time))\n","    \"\"\"\n","\n","    # TODO: insert your code here\n","    \n","    words = words.map(lambda word: word.lower()).filter(lambda word: word not in [\"\", ''])\n","    mapped_words = words.map(lambda word: (word, 1))\n","    reduced_words = mapped_words.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 10, 10)    \n","    total_word_count = reduced_words.transform(lambda time, wC: wC.map(lambda x: (x[0], x[1],\n","                                                                                   time.strftime(\"%m/%d %H:%M:%S\"))))\n","    \n","    return total_word_count\n","\n","\n","def savingTry(words, project_id, output_dataset, output_table_wordcount, columns_name_wordcount):\n","    \n","    data = words.collect()\n","    dfData = pd.DataFrame(data, columns=columns_name_wordcount)\n","    sTablePath = \"{}.{}\".format(output_dataset, output_table_wordcount)\n","    dfData.to_gbq(sTablePath, project_id, if_exists='append')\n","    \n","\n","\n","if __name__ == '__main__':\n","    # Spark settings\n","    conf = SparkConf()\n","    conf.setMaster('local[2]')\n","    conf.setAppName(\"StackOverflowStreamApp\")\n","\n","    # create spark context with the above configuration\n","    # sc = SparkContext(conf=conf)\n","    sc = SparkContext.getOrCreate(conf=conf)\n","    sc.setLogLevel(\"ERROR\")\n","\n","    # create sql context, used for saving rdd\n","    sql_context = SQLContext(sc)\n","\n","    # create the Streaming Context from the above spark context with batch interval size 5 seconds\n","    ssc = StreamingContext(sc, 10)\n","    \n","    # setting a checkpoint to allow RDD recovery\n","    ssc.checkpoint(\"~/checkpoint_stackOverflowApp\")\n","\n","    # read data from port 9001\n","    dataStream = ssc.socketTextStream(IP, PORT)\n","    dataStream.pprint()\n","    \n","    words = dataStream.flatMap(lambda line: line.split(\" \"))\n","    wordCount = wordCount(words)\n","    wordCount.pprint()\n","\n","    wordCount.foreachRDD(lambda rdd: saveToStorage(rdd, output_directory_wordcount, columns_name_wordcount, mode=\"overwrite\"))\n","    \n","    # put the temp result in google storage to google BigQuery\n","    wordCount.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","    \n","    # start streaming process, wait for 600s and then stop.\n","    ssc.start()\n","    time.sleep(STREAMTIME)\n","    ssc.stop(stopSparkContext=False, stopGraceFully=True)"]},{"cell_type":"code","execution_count":null,"id":"317e0a30","metadata":{"id":"317e0a30"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b7278a54","metadata":{"id":"b7278a54"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}
