{"cells":[{"cell_type":"code","execution_count":1,"id":"37c438a6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas-gbq in /opt/conda/miniconda3/lib/python3.8/site-packages (0.18.1)\n","Requirement already satisfied: numpy>=1.16.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (1.19.5)\n","Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=2.16.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (2.16.2)\n","Requirement already satisfied: pyarrow>=3.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (9.0.0)\n","Requirement already satisfied: google-auth>=2.13.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (2.15.0)\n","Requirement already satisfied: pandas>=1.1.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (1.2.5)\n","Requirement already satisfied: setuptools in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (59.8.0)\n","Requirement already satisfied: pydata-google-auth>=1.4.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (1.4.0)\n","Requirement already satisfied: google-auth-oauthlib>=0.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (0.7.1)\n","Requirement already satisfied: google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (3.4.0)\n","Requirement already satisfied: google-api-core<3.0.0dev,>=2.10.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (2.11.0)\n","Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas-gbq) (1.0.4)\n","Requirement already satisfied: packaging>=17.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from db-dtypes<2.0.0,>=1.0.4->pandas-gbq) (21.3)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2.25.1)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (1.56.4)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (3.20.3)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth>=2.13.0->pandas-gbq) (4.9)\n","Requirement already satisfied: six>=1.9.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth>=2.13.0->pandas-gbq) (1.16.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth>=2.13.0->pandas-gbq) (0.2.7)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth>=2.13.0->pandas-gbq) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib>=0.7.0->pandas-gbq) (1.3.1)\n","Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (2.3.2)\n","Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.3.3)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.22.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.50.0)\n","Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (2.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas>=1.1.4->pandas-gbq) (2022.6)\n","Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (1.48.2)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from packaging>=17.0->db-dtypes<2.0.0,>=1.0.4->pandas-gbq) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.13.0->pandas-gbq) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (1.25.11)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2022.9.24)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.7.0->pandas-gbq) (3.2.2)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: pyarrow in /opt/conda/miniconda3/lib/python3.8/site-packages (9.0.0)\n","Requirement already satisfied: numpy>=1.16.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyarrow) (1.19.5)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["! pip install pandas-gbq\n","! pip install pyarrow "]},{"cell_type":"code","execution_count":2,"id":"08b34ae9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/12/06 05:31:48 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/12/06 05:31:48 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/12/06 05:31:48 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/12/06 05:31:48 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","22/12/06 05:32:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304720000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:32:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304730000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:32:20 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304740000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:32:24 WARN org.apache.spark.streaming.receiver.ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n","22/12/06 05:32:24 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n","22/12/06 05:32:25 WARN org.apache.spark.storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n","22/12/06 05:32:25 WARN org.apache.spark.storage.BlockManager: Block input-0-1670304744800 replicated to only 0 peer(s) instead of 1 peers\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["                                               title  \\\n","0  Convert a list&lt;Strings&gt; into a list&lt;M...   \n","\n","                                                body  \n","0   <p>Consider this:</p><pre><code>List&lt;Strin...  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 1911.72it/s]                     (0 + 1) / 1]\n","22/12/06 05:32:37 WARN org.apache.spark.streaming.receiver.ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n","22/12/06 05:32:37 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n","22/12/06 05:32:37 WARN org.apache.spark.storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n","22/12/06 05:32:37 WARN org.apache.spark.storage.BlockManager: Block input-0-1670304757400 replicated to only 0 peer(s) instead of 1 peers\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["                                               title  \\\n","0  How to convert value(Color string) return by f...   \n","\n","                                                body  \n","0   <p>I am having a flutter application and web ...  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 8160.12it/s]                     (0 + 1) / 1]\n","22/12/06 05:32:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304770000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:33:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304780000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:33:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304790000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:33:12 WARN org.apache.spark.streaming.receiver.ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n","22/12/06 05:33:12 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n","22/12/06 05:33:12 WARN org.apache.spark.storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n","22/12/06 05:33:12 WARN org.apache.spark.storage.BlockManager: Block input-0-1670304792600 replicated to only 0 peer(s) instead of 1 peers\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["                                               title  \\\n","0  how to pass input values to a funtion in react...   \n","\n","                                                body  \n","0   <p>i have 2 inputs <code>email</code> and <co...  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]                    (0 + 1) / 1]\n","22/12/06 05:33:25 WARN org.apache.spark.streaming.receiver.ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n","22/12/06 05:33:25 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n","22/12/06 05:33:25 WARN org.apache.spark.storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n","22/12/06 05:33:25 WARN org.apache.spark.storage.BlockManager: Block input-0-1670304805000 replicated to only 0 peer(s) instead of 1 peers\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["                                    title  \\\n","0  Trying to center a paragraph in a div    \n","\n","                                                body  \n","0   <p>I'm very new to html so there are a few th...  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 7767.23it/s]                     (0 + 1) / 1]\n","22/12/06 05:33:40 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304820000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:33:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304830000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:33:58 WARN org.apache.spark.streaming.receiver.ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n","22/12/06 05:33:58 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n","22/12/06 05:33:58 WARN org.apache.spark.storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n","22/12/06 05:33:58 WARN org.apache.spark.storage.BlockManager: Block input-0-1670304838400 replicated to only 0 peer(s) instead of 1 peers\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["                                               title  \\\n","0  How to run FZF in vim in a directory where the...   \n","\n","                                                body  \n","0   <p>I'd like to run fzf file finder on a custo...  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]                     (0 + 1) / 1]\n","22/12/06 05:34:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304850000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:34:11 WARN org.apache.spark.streaming.receiver.ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n","22/12/06 05:34:11 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n","22/12/06 05:34:11 WARN org.apache.spark.storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n","22/12/06 05:34:11 WARN org.apache.spark.storage.BlockManager: Block input-0-1670304851000 replicated to only 0 peer(s) instead of 1 peers\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["                          title  \\\n","0  Translator API for react js    \n","\n","                                                body  \n","0   <p>I want to use a translator like a google t...  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 9686.61it/s]                     (0 + 1) / 1]\n","22/12/06 05:34:30 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304870000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:34:34 WARN org.apache.spark.streaming.receiver.ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n","22/12/06 05:34:34 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n","22/12/06 05:34:34 WARN org.apache.spark.storage.RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n","22/12/06 05:34:34 WARN org.apache.spark.storage.BlockManager: Block input-0-1670304874200 replicated to only 0 peer(s) instead of 1 peers\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["                                               title  \\\n","0  How to create a list such that each element of...   \n","\n","                                                body  \n","0   <p>I have an example dataframe s1</p><pre><co...  \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 11554.56it/s]                    (0 + 1) / 1]\n","22/12/06 05:34:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304890000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:35:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1670304900000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 568, in _list_to_arrays\n","    columns = _validate_or_indexify_columns(content, columns)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 692, in _validate_or_indexify_columns\n","    raise AssertionError(\n","AssertionError: 2 columns passed, passed data had 1 columns\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 142, in <lambda>\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","  File \"/tmp/ipykernel_17641/1230422357.py\", line 102, in savingTry\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 570, in __init__\n","    arrays, columns = to_arrays(data, columns, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 528, in to_arrays\n","    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 571, in _list_to_arrays\n","    raise ValueError(e) from e\n","ValueError: 2 columns passed, passed data had 1 columns\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/12/06 05:35:02 ERROR org.apache.spark.streaming.scheduler.ReceiverTracker: Receiver has been stopped. Try to restart it.\n","org.apache.spark.SparkException: Job 0 cancelled as part of cancellation of all jobs\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2200)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1017)\n","\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n","\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n","\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1016)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2456)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)","Cell \u001B[0;32mIn [2], line 148\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;66;03m# saveToBigQuery(sc, output_dataset, output_table_wordcount, output_directory_wordcount)\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# start streaming process, wait for 600s and then stop.\u001B[39;00m\n\u001B[1;32m    147\u001B[0m ssc\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m--> 148\u001B[0m \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSTREAMTIME\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m ssc\u001B[38;5;241m.\u001B[39mstop(stopSparkContext\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, stopGraceFully\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/context.py:285\u001B[0m, in \u001B[0;36mSparkContext._do_init.<locals>.signal_handler\u001B[0;34m(signal, frame)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msignal_handler\u001B[39m(signal, frame):\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancelAllJobs()\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m()\n","\u001B[0;31mKeyboardInterrupt\u001B[0m: "]}],"source":["from pyspark import SparkConf,SparkContext\n","from pyspark.streaming import StreamingContext\n","from pyspark.sql import Row,SQLContext\n","import sys\n","import requests\n","import time\n","import subprocess\n","import re\n","from google.cloud import bigquery\n","import pandas as pd\n","import pyarrow\n","\n","# global variables\n","bucket = \"eecs6893-stackoverflow-title-body-bucket\"    # TODO : replace with your own bucket name\n","output_directory_wordcount = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/wordcount'.format(bucket)\n","\n","# output table and columns name\n","project_id = 'big-data-6893-363219'\n","output_dataset = 'stackoverflow_title_body_output'                     #the name of your dataset in BigQuery\n","output_table_wordcount = 'title_body'\n","columns_name_wordcount = ['title', 'body']\n","\n","# parameter\n","IP = 'localhost'    # ip port\n","PORT = 9001         # port\n","\n","STREAMTIME = 600     # time that the streaming process runs\n","\n","# Helper functions\n","def saveToStorage(rdd, output_directory, columns_name, mode):\n","    \"\"\"\n","    Save each RDD in this DStream to google storage\n","    Args:\n","    rdd: input rdd\n","    output_directory: output directory in google storage\n","    columns_name: columns name of dataframe\n","    mode: mode = \"overwirte\", overwirte the file\n","    mode = \"append\", append data to the end of file\n","    \"\"\"\n","    if not rdd.isEmpty():\n","        (rdd.toDF( columns_name ) \\\n","        .write.save(output_directory, format=\"json\", mode=mode))\n","        \n","def saveToBigQuery(sc, output_dataset, output_table, directory):\n","    \"\"\"\n","    Put temp streaming json files in google storage to google BigQuery\n","    and clean the output files in google storage\n","    \"\"\"\n","    files = directory + '/part-*'\n","    subprocess.check_call(\n","        'bq load --source_format NEWLINE_DELIMITED_JSON '\n","        '--replace '\n","        '--autodetect '\n","        '{dataset}.{table} {files}'.format(\n","            dataset=output_dataset, table=output_table, files=files\n","        ).split())\n","    output_path = sc._jvm.org.apache.hadoop.fs.Path(directory)\n","    output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(output_path, True)\n","\n","def wordCount(words):\n","    \"\"\"\n","    Calculte the count of 5 sepcial words for every 60 seconds (window no overlap)\n","    You can choose your own words.\n","    Your should:\n","    1. filter the words\n","    2. count the word during a special window size\n","    3. add a time related mark to the output of each window, ex: a datetime type\n","    Hints:\n","        You can take a look at reduceByKeyAndWindow transformation\n","        Dstream is a serioes of rdd, each RDD in a DStream contains data from a certain interval\n","        You may want to take a look of transform transformation of DStream when trying to add a time\n","    Args:\n","        dstream(DStream): stream of real time tweets\n","    Returns:\n","        DStream Object with inner structure (word, (count, time))\n","    \"\"\"\n","\n","    # TODO: insert your code here\n","    \n","    # i don't need this \n","    #words = words.map(lambda word: word.lower()).filter(lambda word: word not in [\"\", ''])\n","    mapped_words = words.map(lambda word: (word, 1))\n","    reduced_words = mapped_words.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 10, 10)    \n","    total_word_count = reduced_words.transform(lambda time, wC: wC.map(lambda x: (x[0], x[1],\n","                                                                                   time.strftime(\"%m/%d %H:%M:%S\"))))\n","    \n","    return total_word_count\n","\n","def savingTry(words, project_id, output_dataset, output_table_wordcount, columns_name_wordcount):\n","    \n","#     words = words.transform(lambda time, x: (x[0], x[1], time.strftime(\"%m/%d %H:%M:%S\")))\n","    \n","    data = words.collect()\n","    \n","    # 1st scanerio to consider: what if there are multiple questions \n","    data[1 : ] = [''.join(data[1 : ])]\n","    \n","    list_of_list = []\n","    list_of_list.append(data)\n","    \n","    #print(data)\n","    dfData = pd.DataFrame(list_of_list, columns=columns_name_wordcount)\n","    print(dfData)\n","    sTablePath = \"{}.{}\".format(output_dataset, output_table_wordcount)\n","    dfData.to_gbq(sTablePath, project_id, if_exists='append')\n","    \n","    \n","if __name__ == '__main__':\n","    # Spark settings\n","    conf = SparkConf()\n","    conf.setMaster('local[2]')\n","    conf.setAppName(\"StackOverflowStreamApp\")\n","\n","    # create spark context with the above configuration\n","    # sc = SparkContext(conf=conf)\n","    sc = SparkContext.getOrCreate(conf=conf)\n","    sc.setLogLevel(\"ERROR\")\n","\n","    # create sql context, used for saving rdd\n","    sql_context = SQLContext(sc)\n","\n","    # create the Streaming Context from the above spark context with batch interval size 5 seconds\n","    ssc = StreamingContext(sc, 10)\n","    \n","    # setting a checkpoint to allow RDD recovery\n","    ssc.checkpoint(\"~/checkpoint_stackOverflowApp\")\n","\n","    # read data from port 9001\n","    dataStream = ssc.socketTextStream(IP, PORT)\n","#     dataStream.pprint()\n","    \n","    #print(type(dataStream))\n","    \n","    \n","    words = dataStream.flatMap(lambda line: line.split(\"|||||||\"))\n","#     wordCount = wordCount(words)\n","    words.pprint() # this is working\n","\n","#     wordCount.foreachRDD(lambda rdd: saveToStorage(rdd, output_directory_wordcount, columns_name_wordcount, mode=\"append\"))\n","    \n","    # put the temp result in google storage to google BigQuery\n","    words.foreachRDD(lambda rdd: savingTry(rdd, project_id, output_dataset, output_table_wordcount, columns_name_wordcount))\n","    \n","    # saveToBigQuery(sc, output_dataset, output_table_wordcount, output_directory_wordcount)\n","    \n","    # start streaming process, wait for 600s and then stop.\n","    ssc.start()\n","    time.sleep(STREAMTIME)\n","    ssc.stop(stopSparkContext=False, stopGraceFully=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}